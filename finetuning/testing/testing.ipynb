{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ElectraConfig, ElectraForSequenceClassification, PreTrainedTokenizerFast, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the configuration from a JSON file\n",
    "config = ElectraConfig.from_json_file(\"/Users/wejarrard/projects/atacToChip/finetuning/preprocessing/output/discriminator.json\")\n",
    "config.num_labels = 1  # Adjust the number of output labels\n",
    "\n",
    "# Initialize the model\n",
    "model = ElectraForSequenceClassification(config)\n",
    "\n",
    "# Identify the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the path where you saved the model\n",
    "save_path = 'best_model'\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "\n",
    "# Make sure to call model.to(device) to ensure that the model's parameters are on the right device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_path):\n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=tokenizer_path, max_len=512)\n",
    "    return tokenizer\n",
    "\n",
    "class GenomicsDataset(Dataset):\n",
    "    def __init__(self, dir_label_dict, min_val, max_val, dataset=\"train\", tokenizer=get_tokenizer(os.path.join(\"/Users/wejarrard/projects/atacToChip/finetuning/preprocessing/output\", 'tokenizer.json'))):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        self.dataset = dataset\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        for dir_path, label in dir_label_dict.items():\n",
    "            files = [os.path.join(dir_path, file) for file in os.listdir(dir_path) if not pd.read_feather(os.path.join(dir_path, file)).empty]\n",
    "            self.file_paths += files\n",
    "            self.labels += [label] * len(files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        df = pd.read_feather(file_path)\n",
    "\n",
    "        # Seperate the data into the sequence, chromosome, and read counts\n",
    "        if self.dataset == \"train\":\n",
    "            # Introduce a small random cut\n",
    "            cut = random.randint(0, 10)\n",
    "\n",
    "            # Apply the cut\n",
    "            dna_sequence = df[\"base\"].str.cat(sep=\"\")[cut:]\n",
    "            read_counts = torch.tensor(df[\"reads\"].values.astype(np.float64))[cut:]\n",
    "            positions = torch.tensor(df[\"pos\"].values.astype(np.int64))[cut:]\n",
    "        else:\n",
    "            dna_sequence = df[\"base\"].str.cat(sep=\"\")\n",
    "            read_counts = torch.tensor(df[\"reads\"].values.astype(np.float64))\n",
    "            positions = torch.tensor(df[\"pos\"].values.astype(np.int64))\n",
    "        chromosome = df[\"chrom\"].values[0]\n",
    "        # Normalize the read counts\n",
    "        read_counts = self.min_max_norm(read_counts, self.min_val, self.max_val)\n",
    "\n",
    "        tokenized_sequence = self.tokenizer.encode(dna_sequence, return_tensors=\"pt\")\n",
    "\n",
    "        cur_pos = 0\n",
    "\n",
    "        final_read_counts = torch.zeros(len(tokenized_sequence[0]))\n",
    "        final_positions = torch.zeros(len(tokenized_sequence[0]))\n",
    "\n",
    "        for i in range(len(tokenized_sequence[0])):\n",
    "\n",
    "            # Get the token\n",
    "            token = tokenized_sequence[0][i]\n",
    "\n",
    "            # Get length of token\n",
    "            token_length = len(self.tokenizer.decode(token))\n",
    "\n",
    "            # Get the read counts for the token\n",
    "            token_read_counts = read_counts[cur_pos:cur_pos+token_length]\n",
    "\n",
    "            token_positions = positions[cur_pos:cur_pos+token_length]\n",
    "\n",
    "            # Get the average read count for the token\n",
    "            token_read_count = token_read_counts.mean()\n",
    "\n",
    "            #get the beginning of the token position\n",
    "            token_position = token_positions.min()\n",
    "\n",
    "            # Replace the read counts for the token with the average read count\n",
    "            final_read_counts[i] = token_read_count\n",
    "\n",
    "            final_positions[i] = token_position\n",
    "\n",
    "            # Update the current position\n",
    "            cur_pos += token_length\n",
    "\n",
    "        # Add batch dimension\n",
    "        final_read_counts = torch.unsqueeze(final_read_counts, 0)\n",
    "        final_positions = torch.unsqueeze(final_positions, 0)\n",
    "\n",
    "        # Get the tokenized sequence, chromosome, read counts and position for the 512 tokens if the sequence is longer than 512\n",
    "        if len(tokenized_sequence[0]) > 512:  \n",
    "            if self.dataset == \"train\":\n",
    "                start = random.randint(0, len(tokenized_sequence[0]) - 512)\n",
    "                tokenized_sequence = tokenized_sequence[:, start:start+512]\n",
    "                final_read_counts = final_read_counts[:, start:start+512]\n",
    "                final_positions = final_positions[:, start:start+512]\n",
    "            else:\n",
    "                # Get the middle 512 tokens\n",
    "                start = (len(tokenized_sequence[0]) - 512) // 2\n",
    "                tokenized_sequence = tokenized_sequence[:, start:start+512]\n",
    "                final_read_counts = final_read_counts[:, start:start+512]\n",
    "                final_positions = final_positions[:, start:start+512]\n",
    "\n",
    "\n",
    "        elif len(tokenized_sequence[0]) < 512:\n",
    "\n",
    "            # Pad the tokenized sequence, masked tokens, read counts and position with 0s\n",
    "            tokenized_sequence = torch.nn.functional.pad(\n",
    "                tokenized_sequence, (0, 512 - len(tokenized_sequence[0])))\n",
    "            final_read_counts = torch.nn.functional.pad(\n",
    "                final_read_counts, (0, 512 - len(final_read_counts[0])))\n",
    "            final_positions = torch.nn.functional.pad(\n",
    "                final_positions, (0, 512 - len(final_positions[0])))\n",
    "\n",
    "        # create position tensor\n",
    "        position = torch.arange(0, 512, 1)\n",
    "\n",
    "        # Create chromosome tensor\n",
    "        chromosome = torch.full((1, 512), chromosome)\n",
    "\n",
    "        # Add batch dimension\n",
    "        position = torch.unsqueeze(position, 0)\n",
    "\n",
    "\n",
    "        tokenized_sequence = tokenized_sequence.squeeze(1)\n",
    "        position = position.squeeze(1)\n",
    "        chromosome = chromosome.squeeze(1)\n",
    "        final_read_counts = final_read_counts.squeeze(1)\n",
    "        final_positions = final_positions.squeeze(1)\n",
    "\n",
    "        tokenized_sequence = tokenized_sequence.squeeze(0)\n",
    "        position = position.squeeze(0)\n",
    "        chromosome = chromosome.squeeze(0)\n",
    "        final_read_counts = final_read_counts.squeeze(0)\n",
    "        final_positions = final_positions.squeeze(0)\n",
    "\n",
    "        # Return the tokenized sequence, chromosome, read counts and position\n",
    "        return {\n",
    "            'input_ids': tokenized_sequence, \n",
    "            'position_ids': position, \n",
    "            'chromosome': chromosome, \n",
    "            'location': final_positions,\n",
    "            'reads': final_read_counts, \n",
    "            'labels': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "    def min_max_norm(self, x, min_val, max_val):\n",
    "        for i in range(len(x)):\n",
    "            x[i] = (x[i] - min_val) / (max_val - min_val)\n",
    "            if x[i] < 0:\n",
    "                x[i] = 0\n",
    "            if x[i] > 1:\n",
    "                x[i] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "min_val, max_val = 0, 366.0038259577389\n",
    "data_dir = \"/Users/wejarrard/projects/atacToChip/finetuning/testing/output/test\"\n",
    "\n",
    "dir_label_dict = {\n",
    "    os.path.join(data_dir, \"atacseq_only\"): 0,\n",
    "    # os.path.join(data_dir, \"chipseq_only\"): 1,\n",
    "    os.path.join(data_dir, \"intersecting\"): 1\n",
    "}\n",
    "\n",
    "valid_dataset = GenomicsDataset(dir_label_dict, min_val, max_val, dataset='valid')\n",
    "\n",
    "# create a DataLoader for the dataset\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize lists to store predictions, labels, and locations\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_locations = []\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for all batches in the validation dataset\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels' and k != 'chromosome' and k != 'location'}\n",
    "        outputs = model(**inputs)\n",
    "        prob = torch.sigmoid(outputs.logits)\n",
    "        pred_labels = torch.round(prob).cpu().numpy()\n",
    "        all_preds.extend(pred_labels)\n",
    "        \n",
    "        true_labels = batch['labels'].cpu().numpy()\n",
    "        all_labels.extend(true_labels)\n",
    "\n",
    "        locations = batch['location'].cpu().numpy()\n",
    "        all_locations.extend(locations)\n",
    "\n",
    "# Create lists to store correctly and incorrectly predicted locations\n",
    "correct_locations = []\n",
    "incorrect_locations = []\n",
    "\n",
    "# Compare predicted and true labels and store locations\n",
    "for pred, true, loc in zip(all_preds, all_labels, all_locations):\n",
    "    if pred == true:\n",
    "        correct_locations.append(loc)\n",
    "    else:\n",
    "        incorrect_locations.append(loc)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "np.savetxt(\"correct_locations_both.txt\", correct_locations)\n",
    "np.savetxt(\"incorrect_locations_both.txt\", incorrect_locations)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 score: {f1:.2f}')\n",
    "print(f'Correctly predicted locations: {correct_locations}')\n",
    "print(f'Incorrectly predicted locations: {incorrect_locations}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
