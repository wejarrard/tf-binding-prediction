{
    "runs": [
        {
            "number": 1,
            "args": {
                "data_n_tensors_per_file": 2048,
                "data_max_seq_length": 128,
                "distributed_world_size": "torch.cuda.device_count()",
                "gpu": 0,
                "gpu_enabled": "distributed_world_size > 0",
                "gpu_deterministic": "gpu_enabled",
                "gpu_mixed_precision": "gpu_enabled",
                "distributed_port": 8888,
                "distributed_enabled": "distributed_world_size > 1",
                "model_generator": "generator.json",
                "model_discriminator": "discriminator.json",
                "model_mask_prob": 0.15,
                "opt_lr": 0.0005,
                "opt_batch_size": "128 // distributed_world_size if distributed_enabled else 128",
                "opt_warmup_steps": 10000,
                "opt_num_training_steps": 200000,
                "step_log": 20,
                "step_ckpt": 100
            },
            "discriminator": {
                "architectures": [
                    "ElectraForPreTraining"
                ],
                "attention_probs_dropout_prob": 0.1,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.1,
                "hidden_size": 256,
                "initializer_range": 0.02,
                "intermediate_size": 1024,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 4,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "generator": {
                "architectures": [
                    "ElectraForMaskedLM"
                ],
                "attention_probs_dropout_prob": 0.1,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.1,
                "hidden_size": 64,
                "initializer_range": 0.02,
                "intermediate_size": 256,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 1,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            }
        },
        {
            "number": 2,
            "args": {
                "data_n_tensors_per_file": 2048,
                "data_max_seq_length": 128,
                "distributed_world_size": "torch.cuda.device_count()",
                "gpu": 0,
                "gpu_enabled": "distributed_world_size > 0",
                "gpu_deterministic": "gpu_enabled",
                "gpu_mixed_precision": "gpu_enabled",
                "distributed_port": 8888,
                "distributed_enabled": "distributed_world_size > 1",
                "model_generator": "generator.json",
                "model_discriminator": "discriminator.json",
                "model_mask_prob": 0.15,
                "opt_lr": 0.0003,
                "opt_batch_size": "128 // distributed_world_size if distributed_enabled else 128",
                "opt_warmup_steps": 5000,
                "opt_num_training_steps": 200000,
                "step_log": 20,
                "step_ckpt": 100
            },
            "discriminator": {
                "architectures": [
                    "ElectraForPreTraining"
                ],
                "attention_probs_dropout_prob": 0.1,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.1,
                "hidden_size": 256,
                "initializer_range": 0.02,
                "intermediate_size": 1024,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 8,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "generator": {
                "architectures": [
                    "ElectraForMaskedLM"
                ],
                "attention_probs_dropout_prob": 0.1,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.1,
                "hidden_size": 96,
                "initializer_range": 0.02,
                "intermediate_size": 256,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 2,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "note": "Args: 'opt_lr' changed from '0.0005' to '0.0003', 'opt_warmup_steps' changed from '10000' to '5000'; Discriminator: 'num_attention_heads' changed from '4' to '8'; Generator: 'hidden_size' changed from '64' to '96', 'num_attention_heads' changed from '1' to '2'"
        },
        {
            "number": 3,
            "args": {
                "data_n_tensors_per_file": 2048,
                "data_max_seq_length": 128,
                "distributed_world_size": "torch.cuda.device_count()",
                "gpu": 0,
                "gpu_enabled": "distributed_world_size > 0",
                "gpu_deterministic": "gpu_enabled",
                "gpu_mixed_precision": "gpu_enabled",
                "distributed_port": 8888,
                "distributed_enabled": "distributed_world_size > 1",
                "model_generator": "generator.json",
                "model_discriminator": "discriminator.json",
                "model_mask_prob": 0.15,
                "opt_lr": 0.0004,
                "opt_batch_size": "128 // distributed_world_size if distributed_enabled else 128",
                "opt_warmup_steps": 2500,
                "opt_num_training_steps": 200000,
                "step_log": 20,
                "step_ckpt": 100
            },
            "generator": {
                "architectures": [
                    "ElectraForMaskedLM"
                ],
                "attention_probs_dropout_prob": 0.05,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.05,
                "hidden_size": 128,
                "initializer_range": 0.02,
                "intermediate_size": 512,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 2,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "discriminator": {
                "architectures": [
                    "ElectraForPreTraining"
                ],
                "attention_probs_dropout_prob": 0.1,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.1,
                "hidden_size": 256,
                "initializer_range": 0.02,
                "intermediate_size": 1024,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 4,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "note": "Args: 'opt_lr' changed from '0.0003' to '0.0004', 'opt_warmup_steps' changed from '5000' to '2500'; Discriminator: 'num_attention_heads' changed from '8' to '4'; Generator: 'attention_probs_dropout_prob' changed from '0.1' to '0.05', 'hidden_dropout_prob' changed from '0.1' to '0.05', 'hidden_size' changed from '96' to '128', 'intermediate_size' changed from '256' to '512'"
        },
        {
            "number": 4,
            "note": "3rd run generator got to 10 percent then naned, we should keep the same parameters and lengthen the warmup and shrink the LR | Args: 'opt_lr' changed from '0.0004' to '0.0003', 'opt_warmup_steps' changed from '2500' to '5000'; Discriminator: 'attention_probs_dropout_prob' changed from '0.1' to '0.05', 'hidden_dropout_prob' changed from '0.1' to '0.05', 'hidden_size' changed from '256' to '128', 'intermediate_size' changed from '1024' to '512', 'num_attention_heads' changed from '4' to '2'",
            "args": {
                "data_n_tensors_per_file": 2048,
                "data_max_seq_length": 128,
                "distributed_world_size": "torch.cuda.device_count()",
                "gpu": 0,
                "gpu_enabled": "distributed_world_size > 0",
                "gpu_deterministic": "gpu_enabled",
                "gpu_mixed_precision": "gpu_enabled",
                "distributed_port": 8888,
                "distributed_enabled": "distributed_world_size > 1",
                "model_generator": "generator.json",
                "model_discriminator": "discriminator.json",
                "model_mask_prob": 0.15,
                "opt_lr": 0.0003,
                "opt_batch_size": "128 // distributed_world_size if distributed_enabled else 128",
                "opt_warmup_steps": 5000,
                "opt_num_training_steps": 200000,
                "step_log": 20,
                "step_ckpt": 100
            },
            "discriminator": {
                "architectures": [
                    "ElectraForPreTraining"
                ],
                "attention_probs_dropout_prob": 0.05,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.05,
                "hidden_size": 128,
                "initializer_range": 0.02,
                "intermediate_size": 512,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 2,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "generator": {
                "architectures": [
                    "ElectraForMaskedLM"
                ],
                "attention_probs_dropout_prob": 0.05,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.05,
                "hidden_size": 128,
                "initializer_range": 0.02,
                "intermediate_size": 512,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 2,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            }
        },
        {
            "number": 5,
            "args": {
                "data_n_tensors_per_file": 2048,
                "data_max_seq_length": 128,
                "distributed_world_size": "torch.cuda.device_count()",
                "gpu": 0,
                "gpu_enabled": "distributed_world_size > 0",
                "gpu_deterministic": "gpu_enabled",
                "gpu_mixed_precision": "gpu_enabled",
                "distributed_port": 8888,
                "distributed_enabled": "distributed_world_size > 1",
                "model_generator": "generator.json",
                "model_discriminator": "discriminator.json",
                "model_mask_prob": 0.15,
                "opt_lr": 0.0003,
                "opt_batch_size": "128 // distributed_world_size if distributed_enabled else 128",
                "opt_warmup_steps": 5000,
                "opt_num_training_steps": 200000,
                "step_log": 20,
                "step_ckpt": 100
            },
            "discriminator": {
                "architectures": [
                    "ElectraForPreTraining"
                ],
                "attention_probs_dropout_prob": 0.1,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.1,
                "hidden_size": 256,
                "initializer_range": 0.02,
                "intermediate_size": 1024,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 4,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "generator": {
                "architectures": [
                    "ElectraForMaskedLM"
                ],
                "attention_probs_dropout_prob": 0.05,
                "embedding_size": 128,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.05,
                "hidden_size": 128,
                "initializer_range": 0.02,
                "intermediate_size": 512,
                "layer_norm_eps": 1e-12,
                "max_position_embeddings": 512,
                "chromosome_vocab_size": 24,
                "model_type": "electra",
                "num_attention_heads": 2,
                "num_hidden_layers": 12,
                "type_vocab_size": 2,
                "vocab_size": 5000
            },
            "note": "Discriminator: 'attention_probs_dropout_prob' changed from '0.05' to '0.1', 'hidden_dropout_prob' changed from '0.05' to '0.1', 'hidden_size' changed from '128' to '256', 'intermediate_size' changed from '512' to '1024', 'num_attention_heads' changed from '2' to '4'"
        }
    ]
}